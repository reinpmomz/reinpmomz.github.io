---
title: "Predicting Stroke"
output:
  html_document:
    toc: yes
    toc_depth: 4
    number_sections: yes
  word_document:
    toc: yes
---

# Introduction

## Overview

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. Six and a half-million people die from stroke annually(3.3 million from ischaemic stroke annually, 2.9 million from intracerebral haemorrhages and 0.3 million from subarachnoid haemorrhages). One in four people over age 25 will have a stroke in their lifetime. 62% of all incident strokes are ischaemic strokes while 28% are intracerebral haemorrhages and 10% are subarachnoid haemorrhages.


## About the Data

This data set is used to predict whether a patient is likely to get stroke based on some input parameters like gender, age, hypertension, heart diseases, residence, bmi and smoking status. Each row in the data provides relevant information about the patient.

The data dictionary is as follows:

- id: unique identifier
- gender: "Male", "Female" or "Other"
- age: age of the patient
- hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
- heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
- ever_married: "No" or "Yes"
- work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"
- Residence_type: "Rural" or "Urban"
- avg_glucose_level: average glucose level in blood
- bmi: body mass index
- smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*
- stroke: 1 if the patient had a stroke or 0 if not

*Note: "Unknown" in smoking_status means that the information is unavailable for this patient


# Data Exploration

```{r setup, include=FALSE}
## Set Chunk requirements
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

```

## loading Relevant packages

```{r Import relevant packages}
#Import relevant packages

library(tidyverse)
library(janitor)
library(readr)
library(plotly)
library(knitr)


```


## loading Data Set


```{r importing data}
stroke <- read_csv('https://raw.githubusercontent.com/reinpmomz/Data_sets/main/Data/healthcare-dataset-stroke-data.csv', na=c("N/A"))%>%
  clean_names()

```

```{r}
rmarkdown::paged_table(stroke)
```

```{r}
dim(stroke)
```

* Our data has 5110 observations and 12 variables

```{r checking the data structure}
##checking the data structure

str(stroke)

```

From the output on the data structure, all of the data has been read as numeric values('double' value or a decimal type with at least two decimal places) and character values but some should be converted to factors since they are categorical.

## Converting into factors

```{r converting categorical to factors}

stroke_final <- stroke%>%
  filter(gender!= "Other")%>%
  mutate(gender = factor(gender, levels = c("Female", "Male")))%>%
  mutate(hypertension= factor(hypertension, levels = c(0,1), 
                                      labels = c("No", "Yes")))%>%
  mutate(heart_disease= factor(heart_disease, levels = c(0,1), 
                                      labels = c("No", "Yes")))%>%
  mutate(ever_married = factor(ever_married, levels = c("No", "Yes")))%>%
  mutate(across(c(work_type, residence_type, smoking_status), as.factor))%>%
  mutate(stroke= factor(stroke, levels = c(0,1), 
                                      labels = c("No", "Yes")))%>%
  labelled::set_variable_labels(
    id = "unique identifier",
  gender = "Sex of the patient",
  age = "Age of the patient",
  hypertension = "Patient has hypertension",
  heart_disease = "Patient has heart disease",
  ever_married = "Patient is married",
  work_type = "Work type",
  residence_type = "Residence type",
  avg_glucose_level = "Average glucose level in blood",
  bmi = "Body mass index (in kg/m2)",
  smoking_status = "Smoking status",
  stroke = "Patient had stroke"
  )

```



## checking missing values

```{r missing values}

sum(is.na(stroke_final))

#which(is.na(stroke_final))

#which(!complete.cases(stroke_final))


sapply(stroke_final,function(x) sum(is.na(x)))

```
There were 201 missing values in bmi variable in our data set.

# Exploratory data analysis


## Univariate analysis

This is analysis of one variable to enable us understand the distribution of values for a single variable.

### Normality of continous variables


```{r}
shapiro.test(stroke_final$bmi)
```

The shapiro.test has a restriction in R that it can be applied only up to a sample of size 5000 and the least sample size must be 3. Therefore, we have an alternative hypothesis test called Anderson Darling normality test. To perform this test, we need load nortest package and use the ad.test function

```{r}
nortest::ad.test(stroke_final$age)

```

```{r}
nortest::ad.test(stroke_final$avg_glucose_level)
```


If the p-value < 0.05, it implies that the distribution of the data are significantly different from normal distribution. In other words, we cannot assume the normality.


### Descriptives Frequency table

```{r Import relevant packages2}
library(gtsummary)
library(flextable)

set_gtsummary_theme(list(
  "tbl_summary-fn:percent_fun" = function(x) style_percent(x, digits = 1),
  "tbl_summary-str:categorical_stat" = "{n} ({p}%)"
))
# Setting `Compact` theme
theme_gtsummary_compact()

```


```{r descriptives}
# make dataset with variables to summarize

      
tbl_summary(stroke_final%>%
              dplyr::select(-id),
                      type = list(
                        all_dichotomous() ~ "categorical",
                         all_continuous() ~ "continuous2")
                      , statistic = all_continuous() ~ c(
                                     "{mean} ({sd})", 
                                     "{median} ({p25}, {p75})", 
                                     "{min}, {max}")
                      , digits = all_continuous() ~ 2
                      , missing = "always" # don't list missing data separately
                      ,missing_text = "Missing"
                      ) %>% 
  modify_header(label = "**Descriptives**") %>% # update the column header
  bold_labels() %>%
  italicize_levels()%>%
  add_n() # add column with total number of non-missing observations


```

### Visualization



```{r bar graphs1, out.width="50%",fig.show="hold"}
library(ggpubr)


 ggplot(stroke_final, aes(x=stroke))+
  geom_bar(aes(fill = stroke), show.legend = FALSE)+
  labs(x="",y="", title = "Patient had stroke")+
  geom_text(aes(label = paste0(..count.., " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = 1.1, colour = "black")+
  #guides(fill = FALSE)+
  theme_pubclean()+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        #legend.position = "none",
        axis.text.y = element_text(color="black",size=12),
        axis.text.x = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())


 ggplot(stroke_final, aes(x=gender))+
  geom_bar(aes(fill = gender), show.legend = FALSE)+
  labs(x="",y="", title = "Sex of the patient")+
  scale_y_continuous(breaks = seq(0, 3000, by = 500), limits = c(0, 3000))+
  theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .01), ")"), group=1),
            stat = "count", vjust = -0.4, colour = "black")+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())

 ggplot(stroke_final, aes(x=hypertension))+
  geom_bar(aes(fill = hypertension), show.legend = FALSE)+
  labs(x="",y="", title = "Patient has hypertension")+
  scale_y_continuous(breaks = seq(0, 5000, by = 1000), limits = c(0, 5000))+
  theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = 1.1, colour = "black")+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())

 ggplot(stroke_final, aes(x=heart_disease))+
  geom_bar(aes(fill = heart_disease), show.legend = FALSE)+
  labs(x="",y="", title = "Patient has heart disease")+
  scale_y_continuous(breaks = seq(0, 5000, by = 1000), limits = c(0, 5000))+
  theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = 1.1, colour = "black")+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        axis.text.y = element_text(color="black",size=12),
        axis.text.x = element_text(color="black",size=12, angle = 0),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())
 
 
 ggplot(stroke_final, aes(x=ever_married))+
  geom_bar(aes(fill = ever_married), show.legend = FALSE)+
  labs(x="",y="", title = "Patient is married")+
  scale_y_continuous(breaks = seq(0, 3500, by = 500), limits = c(0, 3500))+
 theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = 1.1, colour = "black")+
  theme(axis.title= element_text(face="bold",color="black",size=13),
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())


 ggplot(stroke_final, aes(x=work_type))+
  geom_bar(aes(fill = work_type), show.legend = FALSE)+
  labs(x="",y="", title = "Work type")+
  scale_y_continuous(breaks = seq(0, 3100, by = 500), limits = c(0, 3100))+
  theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = -0.4, colour = "black")+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        axis.text.y = element_text(color="black",size=12),
        axis.text.x = element_text(color="black",size=11, angle = 0),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())


 ggplot(stroke_final, aes(x=residence_type))+
  geom_bar(aes(fill = residence_type), show.legend = FALSE)+
  labs(x="",y="", title = "Residence type")+
  scale_y_continuous(breaks = seq(0, 2800, by = 400), limits = c(0, 2800))+
  theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = 1.5, colour = "black")+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())


 ggplot(stroke_final, aes(x=smoking_status))+
  geom_bar(aes(fill = smoking_status), show.legend = FALSE)+
  labs(x="",y="", title = "Smoking status")+
  scale_y_continuous(breaks = seq(0, 2000, by = 400), limits = c(0, 2000))+
  theme_pubclean()+
  geom_text(aes(label = paste0(after_stat(count), " (", scales::percent(after_stat(prop), accuracy = .1), ")"), group=1),
            stat = "count", vjust = 1.5, colour = "black")+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        axis.text.y = element_text(color="black",size=12),
        axis.text.x = element_text(color="black",size=11, angle = 0),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank())



```


Majority of the patients were Female (58.6%), married (65.6%), in private employment (57.2%), had no hypertension (90.3%), had no heart disease (94.6%) and had never smoked (37.0%). There was similar proportion in rural and urban residence type.


```{r histograms, out.width="50%",fig.show="hold"}

ggplot(stroke_final, aes(x=age)) + 
  geom_histogram(color="black", fill="lightblue")+
  geom_vline(aes(xintercept=mean(age)),
            color="blue", linetype="dashed", size=1)+
   scale_y_continuous(breaks = seq(0, 300, by = 50), limits = c(0, 300))+
  scale_x_continuous(n.breaks = 10)+
  labs(x="Age of the Patient",y="count", title = "Age (Years)")+
  theme(axis.title = element_text(color="black",size=12),
        axis.text = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())

ggplot(stroke_final, aes(x=avg_glucose_level)) + 
  geom_histogram(color="black", fill="pink")+
  geom_vline(aes(xintercept=mean(avg_glucose_level)),
            color="blue", linetype="dashed", size=1)+
  scale_y_continuous(breaks = seq(0, 700, by = 100), limits = c(0, 700))+
  scale_x_continuous(n.breaks = 12)+
  labs(x="Average glucose level in blood",y="count", title = "Glucose level in blood")+
  theme(axis.title = element_text(color="black",size=12),
        axis.text = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())

ggplot(stroke_final%>%
         drop_na(bmi), aes(x=bmi)) + 
  geom_histogram(color="black", fill="lightgreen")+
  geom_vline(aes(xintercept=mean(bmi)),
            color="blue", linetype="dashed", size=1)+
    scale_y_continuous(breaks = seq(0, 900, by = 100), limits = c(0, 900))+
  scale_x_continuous(n.breaks = 10)+
  labs(x="Body mass index (in kg/m2)",y="count", title = "BMI")+
  theme(axis.title = element_text(color="black",size=12),
        axis.text = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())



```


## Bivariate analysis

### Correlation

Pearson correlation evaluates the linear relationship between two continuous variables.

```{r, fig.width=14, fig.height=10}
# improved correlation matrix
library(corrplot)

corrplot(cor(stroke_final%>%
               dplyr::select(-id)%>%
               mutate(across(c(1:11), as.numeric)),
             method='pearson', use = "complete.obs"),
  method = "color", #number
  addCoef.col = "black",
  number.cex = 0.95,
  type = "upper" # show only upper side #full
)
```


The variables **gender**, **age**, **hypertension**, **heart disease**, **ever married**, **work type**, **residence type**, **average glucose level** and **bmi** exhibit a positive linear association with our target variable (stroke) while **smoking status** a negative linear association.


```{r}
# correlation tests for whole dataset
library(Hmisc)
res <- rcorr(as.matrix(stroke_final%>%
               dplyr::select(-id)%>%
         drop_na(bmi)%>%
               mutate(across(c(1:11), as.numeric)))) # rcorr() accepts matrices only

# display p-values (rounded to 3 decimals)
kable(
  round(res$P, 3)
)
```


The linear association of stroke with **gender** and **residence type** is not significant (p>0.05). 


### Difference Frequency table


```{r}

# make dataset with variables to summarize

tbl_summary(stroke_final%>%
               dplyr::select(-id),
              by = stroke,
                       type = list(
                         all_dichotomous() ~ "categorical",
                          all_continuous() ~ "continuous2")
                       , statistic = all_continuous() ~ c(
                                      "{mean} ({sd})", 
                                      "{median} ({p25}, {p75})", 
                                      "{min}, {max}")
                       , digits = all_continuous() ~ 2
                       , missing = "always" # don't list missing data separately
                       ,missing_text = "Missing"
                       ) %>% 
   modify_header(label = "**Variables**") %>% # update the column header
   bold_labels() %>%
   italicize_levels()%>%
   add_n()%>% # add column with total number of non-missing observations
   add_p(pvalue_fun = ~style_pvalue(.x, digits = 3),
         test.args = c(work_type) ~ list(simulate.p.value=TRUE)) %>%
   bold_p(t= 0.05) %>% # bold p-values under a given threshold (default 0.05)
   #add_overall() %>%
   #add_difference() %>% #add column for difference between two group, confidence interval, and p-value
   modify_spanning_header(c("stat_1", "stat_2") ~ "**Stroke**")  %>%
   #modify_caption("**Table 1. Patient Characteristics**")%>%
   modify_footnote(
     all_stat_cols() ~ "Mean (SD); Median (IQR); Range; Frequency (%)"
   )

```


### Visualization


```{r, out.width="50%",fig.show="hold"}

library(ggthemes)

  ggplot(stroke_final, aes(x=gender))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Sex of the patient", y="Percent", x="")+
  scale_fill_brewer(palette = "Set1")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())
  
  
  ggplot(stroke_final, aes(x=hypertension))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Patient has hypertension", y="Percent", x="")+
  scale_fill_brewer(palette = "Set2")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())
  
  
  ggplot(stroke_final, aes(x=heart_disease))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Patient has heart disease", y="Percent", x="")+
  scale_fill_brewer(palette = "Set3")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())
  
  
  ggplot(stroke_final, aes(x=ever_married))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Patient is married", y="Percent", x="")+
  scale_fill_brewer(palette = "Accent")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())
  
  
  ggplot(stroke_final, aes(x=work_type))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Work type", y="Percent", x="")+
  scale_fill_brewer(palette = "Dark2")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())
  
  
  ggplot(stroke_final, aes(x=residence_type))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Residence type", y="Percent", x="")+
  scale_fill_brewer(palette = "Paired")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())
  
  
  ggplot(stroke_final, aes(x=smoking_status))+
  geom_bar(aes(fill=stroke), position = "fill")+
  labs(title="Smoking status", y="Percent", x="")+
  scale_fill_brewer(palette = "Spectral")+
  scale_y_continuous(breaks=seq(0,1,by=.2),label=scales::percent)+
  theme_fivethirtyeight()+
  theme(axis.title.y = element_blank(),
        #legend.title = element_blank(),
        plot.margin = unit(c(1,1,0,1),"cm"),
        axis.text = element_text(size = 11),
        plot.title = element_text(size=16,hjust = 0.5),
         panel.grid.major.x = element_blank())

```


From the graphs, there is no difference in proportion of stroke with **gender** and **residence type**.


```{r, fig.width=14, fig.height=10}

p1 <- ggplot(stroke_final, aes(x=age, fill=stroke)) + 
  geom_density(alpha=0.4)+
  scale_y_continuous(n.breaks = 10)+
  scale_x_continuous(breaks = seq(0, 85, by = 5), limits = c(0, 85))+
  labs(x="age",y="density", title = "Age of the patient")+
  theme(axis.title = element_text(color="black",size=12),
        legend.position = "top",
        axis.text = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())+
  guides(fill = guide_legend(title = "Stroke"))


p2 <- ggplot(stroke_final, aes(x=avg_glucose_level, fill=stroke)) + 
  geom_density(alpha=0.4)+
  scale_y_continuous(n.breaks = 10)+
  scale_x_continuous(breaks = seq(50, 300, by = 25), limits = c(50, 300))+
  labs(x="Average glucose level in blood",y="density", title = "Glucose level in blood")+
  theme(axis.title = element_text(color="black",size=12),
        legend.position = "top",
        axis.text = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())+
  guides(fill = guide_legend(title = "Stroke"))


p3 <- ggplot(stroke_final, aes(x=bmi, fill=stroke)) + 
  geom_density(alpha=0.4)+
  scale_y_continuous(n.breaks = 10)+
  scale_x_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100))+
  labs(x="Body mass index (in kg/m2)",y="density", title = "BMI")+
  theme(axis.title = element_text(color="black",size=12),
        legend.position = "top",
        axis.text = element_text(color="black",size=11),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())+
  guides(fill = guide_legend(title = "Stroke"))


p4 <- ggplot(stroke_final, aes(stroke, age))+
  geom_boxplot(aes(colour = stroke), outlier.colour = "black", 
               outlier.shape = 8, show.legend = TRUE)+
  labs(x="",y="age", title = "Age of the patient")+
  scale_y_continuous(breaks = seq(0, 90, by = 10), limits = c(0, 90))+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        legend.position = "bottom",
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank())


p5 <- ggplot(stroke_final, aes(stroke, avg_glucose_level))+
  geom_boxplot(aes(colour = stroke), outlier.colour = "black", 
               outlier.shape = 1, show.legend = TRUE)+
  labs(x="",y="Average glucose level in blood", title = "Glucose level in blood")+
  scale_y_continuous(breaks = seq(0, 300, by = 25), limits = c(0, 300))+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        legend.position = "bottom",
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank())


p6 <- ggplot(stroke_final, aes(stroke, bmi))+
  geom_boxplot(aes(colour = stroke), outlier.colour = "black", 
               outlier.shape = 1, show.legend = TRUE)+
  labs(x="",y="Body mass index (in kg/m2)", title = "BMI")+
  scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100))+
  theme(axis.title = element_text(face="bold",color="black",size=13),
        legend.position = "bottom",
        axis.text = element_text(color="black",size=12),
        plot.title = element_text(hjust = 0.5, face="bold",color="black",size=13),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank())


figure1 <- ggarrange( p1, p2, p3, p4, p5, p6, 
          ncol = 3, nrow = 2)

annotate_figure(figure1,
                top = text_grob("",
                                color = "red", face = "bold", size = 15),
                bottom = text_grob("", color = "blue",
                                   hjust = 1, x = 0.98, face = "italic", size = 10),
                #left = text_grob("Figure arranged using ggpubr", color = "green", rot = 90),
                #right = "",
                fig.lab = "", fig.lab.face = "bold"
                )

```


# Modelling

From the exploration done, it is very hard to make a conclusion. The dependent variable “stroke” is imbalanced. Patient had stroke (4.9%) vs Patient who did no have stroke (95.1%). It will be very wrong to just perform a predictive model on such kind of data without making any changes.

Before creating any model, the first step is to drop missing cases and irrelevant variabled (id) then divide the data into training and testing dataset.

```{r}
stroke_model <- stroke_final%>%
              dplyr::select(-id)%>%
  drop_na()

table(stroke_model$stroke)

```


```{r}

library(caTools)

set.seed(123) 

sample <- sample.split(stroke_model$stroke, SplitRatio = 0.8)


train <- subset(stroke_model, sample==TRUE)


test <- subset(stroke_model, sample==FALSE)


#train=stroke_model[sample==TRUE, ]

#test=stroke_model


table(train$stroke)

table(test$stroke)

```


## Random Forest - categorical

### Imbalanced Data

```{r}

library(randomForest)

model1_rf <- randomForest(stroke ~., data = train)
model1_rf



```

```{r}


library(caret)
confusionMatrix(predict(model1_rf, test), test$stroke, positive = "Yes")

```

From the above output we can see that the accuracy of the model is 95.72%. 

However we need to be sure that this model is good by checking the sensitivity and the specificity. Sensitivity is the accuracy of predicting what we are interested in, that is a person having experienced a stroke while specificity is now for no stroke. 

From the output the sensitivity is at 0% while specificity is 100%. This means that this model will be more accurate in predicting those who did not have stroke than those who did. This problem has been caused by the imbalance of the dependent variable. 

This problem can be solved in several ways including; over sampling, undersampling and both. We will use the package called ROSE.

```{r}
library(ROSE)
```


### Oversampling data

We specify N=7518 (3759x2). Over sampling repeats some values in the category with the fewer entries randomly until they are the same with the larger entry. 

```{r}

over_sampling <-ovun.sample(stroke ~.,data=train, method = "over", N=7518, seed=1)$data

table(over_sampling$stroke)

```


```{r}


model2_rf <- randomForest(stroke ~., data = over_sampling)
model2_rf


```

```{r}

confusionMatrix(predict(model2_rf, test), test$stroke, positive = "Yes")

```

From the output above, we can see that sensitivity is 2.38% while specificity has gone down to 99.15%. This proportion is not still good.


### Under sampling data

This reduces the entries in the category with more entries to be the same with the one with few entries hence (167*2=334).

```{r}

under_sampling <-ovun.sample(stroke ~.,data=train, method = "under", N=334, seed=2)$data

table(under_sampling$stroke)

```

```{r}


model3_rf <- randomForest(stroke ~., data = under_sampling)
model3_rf


```



```{r}

confusionMatrix(predict(model3_rf, test), test$stroke, positive = "Yes")

```

From the output above, sensitivity now stands at 76.19% while specificity stands at 72.34%. This is a better model than the unbalanced and over sampled model.

### Both Sampling data

```{r}

both_sampling <- ovun.sample(stroke~.,data=train, method="both",p=0.5, seed=222, N=3926)$data

table(both_sampling$stroke)

```

```{r}


model4_rf <- randomForest(stroke ~., data = both_sampling)
model4_rf


```

```{r}

confusionMatrix(predict(model4_rf, test), test$stroke, positive = "Yes")

```

From the output above, the sensitivity now is at 26.19% while specificity is at 96.92%.

- This means that the under sampled random forest model is the best to predict stroke with test accuracy of 72.51%, sensitivity of 76.19% and specificity of 72.34%.  


## Tree

```{r}
library(tree)
```


### Unpruned classification tree

#### Imbalanced data - all data

```{r}

stroke_tree = tree(stroke ~ ., data = stroke_model)

#stroke_tree = tree(stroke ~ ., data = stroke_model, control = tree.control(nobs = nrow(stroke_model), minsize = 10))

summary(stroke_tree)


```

```{r}

stroke_tree

```

```{r, fig.width=12, fig.height=10}

plot(stroke_tree)
text(stroke_tree, pretty = 0)
title(main = "Unpruned Classification Tree - All data")

```

```{r}
summary(stroke_tree)$used

names(stroke_model)[which(!(names(stroke_model) %in% summary(stroke_tree)$used))]

```


We see this tree has 4 terminal nodes and a misclassification rate of 0.04258. The tree is not using all of the available variables. It has only used 1 variable (age).


#### Imbalanced data - train  

```{r}

stroke_tree_train = tree(stroke ~ ., data = train)


summary(stroke_tree_train)

```


```{r}

stroke_tree_train

```

```{r, fig.width=12, fig.height=10}

plot(stroke_tree_train)
text(stroke_tree_train, pretty = 1)
title(main = "Unpruned Classification Tree - Train data")

```

```{r}
summary(stroke_tree_train)$used

names(train)[which(!(names(train) %in% summary(stroke_tree_train)$used))]

```

This tree is slightly different than the tree fit to all of the data.

We see this tree has 6 terminal nodes and a misclassification rate of 0.04254. The tree is not using all of the available variables. It has only used 3 variables( age, smoking status and average glucose level).


When using the predict() function on a tree, the default type is vector which gives predicted probabilities for both classes. We will use type = class to directly obtain classes.


```{r}

confusionMatrix(predict(stroke_tree_train, train, type = "class"), train$stroke, positive = "Yes")

confusionMatrix(predict(stroke_tree_train, test, type = "class"), test$stroke, positive = "Yes")

```

Here it is easy to see that the tree is not a good fit.  From both outputs the sensitivity is at 0% while specificity is 100%. This means that this model will be more accurate in predicting those who did not have stroke than those who did. This problem has been caused by the imbalance of the dependent variable.


#### Oversampling data


```{r}

stroke_tree_over = tree(stroke ~ ., data = over_sampling)


summary(stroke_tree_over)

```


```{r}

stroke_tree_over

```

```{r, fig.width=12, fig.height=10}

plot(stroke_tree_over)
text(stroke_tree_over, pretty = 1)
title(main = "Unpruned Classification Tree - over sampling data")

```

```{r}
summary(stroke_tree_over)$used

names(over_sampling)[which(!(names(over_sampling) %in% summary(stroke_tree_over)$used))]

```


We see this tree has 9 terminal nodes and a misclassification rate of 0.2264. The tree has only used 4 vaiables (age, smoking status, average glucose level and bmi).

```{r}

confusionMatrix(predict(stroke_tree_over, over_sampling, type = "class"), over_sampling$stroke, positive = "Yes")

confusionMatrix(predict(stroke_tree_over, test, type = "class"), test$stroke, positive = "Yes")

```

Here it is easy to see that the tree is a better fit than the unbalanced train model.  From both outputs the sensitivity has increased to the 90% range while specificity has decreased to 60% range. 

However the tree has been over-fit. The over sampling train set performs much better than the test set.

Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.


#### Under sampling data

```{r}

stroke_tree_under = tree(stroke ~ ., data = under_sampling)


summary(stroke_tree_under)

```


```{r}

stroke_tree_under

```

```{r, fig.width=12, fig.height=10}

plot(stroke_tree_under)
text(stroke_tree_under, pretty = 1)
title(main = "Unpruned Classification Tree - under sampling data")

```


```{r}
summary(stroke_tree_under)$used

names(under_sampling)[which(!(names(under_sampling) %in% summary(stroke_tree_under)$used))]

```


We see this tree has 9 terminal nodes and a misclassification rate of 0.1946. The tree has only used 4 vaiables (age, smoking status, average glucose level and hypertension).

```{r}

confusionMatrix(predict(stroke_tree_under, under_sampling, type = "class"), under_sampling$stroke, positive = "Yes")

confusionMatrix(predict(stroke_tree_under, test, type = "class"), test$stroke, positive = "Yes")

```

Here it is easy to see that the tree is a better fit than the unbalanced train model and over sampled train model (less accuracy).  

However the tree has been over-fit. The under sampling train set performs much better than the test set.


#### Both sampling data

```{r}

stroke_tree_both = tree(stroke ~ ., data = both_sampling)


summary(stroke_tree_both)

```

```{r}

stroke_tree_both

```

```{r, fig.width=12, fig.height=10}

plot(stroke_tree_both)
text(stroke_tree_both, pretty = 1)
title(main = "Unpruned Classification Tree - both sampling data")

```

```{r}
summary(stroke_tree_both)$used

names(both_sampling)[which(!(names(both_sampling) %in% summary(stroke_tree_both)$used))]

```


We see this tree has 9 terminal nodes and a misclassification rate of 0.217. The tree has only used 4 vaiables (age, smoking status, average glucose level and bmi).


```{r}

confusionMatrix(predict(stroke_tree_both, both_sampling, type = "class"), both_sampling$stroke, positive = "Yes")

confusionMatrix(predict(stroke_tree_both, test, type = "class"), test$stroke, positive = "Yes")

```


Here it is easy to see that the tree is a better fit than the unbalanced train model but not a better fit than over sampled (greater accuracy) and under sampled models(greater accuracy).  

However the tree has been over-fit. The both sampling train set performs much better than the test set.


- This means that the under sampled tree model is the best to predict stroke with test accuracy of 66.8%, sensitivity of 80.95% and specificity of 66.17%.


### Pruned classification tree

We will now use cross-validation to find a tree by considering trees of different sizes which have been pruned from our selected tree.

```{r}

set.seed(43)
stroke_tree_under_cv = cv.tree(stroke_tree_under, FUN = prune.misclass)
stroke_tree_under_cv

```


```{r}
# index of tree with minimum error
min_idx = which.min(stroke_tree_under_cv$dev)
min_idx


# number of terminal nodes in that tree
stroke_tree_under_cv$size[min_idx]


# misclassification rate of each tree
stroke_tree_under_cv$dev / nrow(under_sampling)
```

```{r, fig.width=12, fig.height=10}
par(mfrow = c(1, 2))
# default plot
plot(stroke_tree_under_cv)
# better plot
plot(stroke_tree_under_cv$size, stroke_tree_under_cv$dev / nrow(under_sampling), type = "b",
     xlab = "Tree Size", ylab = "CV Misclassification Rate")
```

It appears that a tree of size 5 has the fewest misclassifications of the considered trees, via cross-validation.

We use prune.misclass() to obtain that tree from our selected tree, and plot this smaller tree.


```{r}

stroke_tree_under_prune = prune.misclass(stroke_tree_under, best = 5)
summary(stroke_tree_under_prune)

```


```{r, fig.width=12, fig.height=10}

plot(stroke_tree_under_prune)
text(stroke_tree_under_prune, pretty = 0)
title(main = "Pruned Classification Tree - under sampling data")

```

We again obtain predictions using this smaller tree, and evaluate on the test and train sets.

```{r}

confusionMatrix(predict(stroke_tree_under_prune, under_sampling, type = "class"),
                under_sampling$stroke, positive = "Yes")

confusionMatrix(predict(stroke_tree_under_prune, test, type = "class"), test$stroke, positive = "Yes")

```

- The under sampled pruned tree model has test accuracy of 70.57%, sensitivity of 80.95% and specificity of 70.11%.

- There was an improvement in test set. Pruned under sampled tree model is a better fit than unpruned under sampled tree model. Test Accuracy is higher (70.57% > 66.8%), Test Sensitivity is equal (80.95%) and test Specificity is higher (70.11% > 66.17%)

- It is still obvious that we have over-fit. Trees tend to do this. There are several ways to fix this, including: **bagging, boosting and random forests**.



